{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé• Halalificator - Google Colab Edition\n",
                "\n",
                "**A complete video processing pipeline that:**\n",
                "- üîí **Blurs females** using AI-powered gender detection (CLIP or Caffe models)\n",
                "- üéµ **Removes music** while preserving vocals using Demucs\n",
                "\n",
                "---\n",
                "\n",
                "## Quick Start\n",
                "1. **Run all setup cells** (Sections 1-5) - just click each play button\n",
                "2. **Upload your video** in Section 6\n",
                "3. **Configure options** in Section 7 using the interactive form\n",
                "4. **Run processing** in Section 8\n",
                "5. **Download result** in Section 9\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Environment Check & GPU Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üîß Check Environment { display-mode: \"form\" }\n",
                "#@markdown **Run this cell first!** It checks if you're on Colab and if GPU is available.\n",
                "\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Check if running on Colab\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"üîç ENVIRONMENT CHECK\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\n‚úì Running in Google Colab: {IN_COLAB}\")\n",
                "\n",
                "if IN_COLAB:\n",
                "    # Check GPU\n",
                "    import subprocess\n",
                "    try:\n",
                "        gpu_info = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], text=True)\n",
                "        print(f\"‚úì GPU Available: Yes\")\n",
                "        print(f\"  GPU Info: {gpu_info.strip()}\")\n",
                "    except:\n",
                "        print(\"‚ö†Ô∏è GPU Not Available!\")\n",
                "        print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Select 'T4 GPU'\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Not running in Colab. Some features may not work.\")\n",
                "\n",
                "print(f\"\\n‚úì Python Version: {sys.version.split()[0]}\")\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"‚úÖ Environment check complete!\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Install Dependencies\n",
                "\n",
                "This installs all required Python packages. Takes about 2-3 minutes on first run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üì¶ Install All Dependencies { display-mode: \"form\" }\n",
                "#@markdown This will install: OpenCV, YOLO, CLIP, Demucs, FFmpeg, and more.\n",
                "#@markdown \n",
                "#@markdown **‚è±Ô∏è Takes ~2-3 minutes on first run**\n",
                "\n",
                "print(\"üì¶ Installing dependencies...\\n\")\n",
                "\n",
                "# Install core packages\n",
                "!pip install -q --upgrade pip\n",
                "!pip install -q opencv-python-headless>=4.8.0\n",
                "!pip install -q ultralytics>=8.0.0\n",
                "!pip install -q open-clip-torch>=2.20.0\n",
                "!pip install -q demucs>=4.0.0\n",
                "!pip install -q static-ffmpeg>=2.5\n",
                "!pip install -q torch torchvision torchaudio\n",
                "!pip install -q ipywidgets\n",
                "\n",
                "# Enable widgets in Colab\n",
                "if IN_COLAB:\n",
                "    from google.colab import output\n",
                "    output.enable_custom_widget_manager()\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"‚úÖ All dependencies installed!\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üìö Import Libraries { display-mode: \"form\" }\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "import urllib.request\n",
                "import shutil\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "from static_ffmpeg import add_paths\n",
                "import torch\n",
                "import open_clip\n",
                "from PIL import Image\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display, HTML, clear_output\n",
                "\n",
                "# Verify GPU\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"‚úì PyTorch using device: {device}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# Create directories\n",
                "os.makedirs('inputs', exist_ok=True)\n",
                "os.makedirs('outputs', exist_ok=True)\n",
                "\n",
                "print(\"\\n‚úÖ All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Setup Processing Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üîß Define Helper Functions { display-mode: \"form\" }\n",
                "#@markdown Model download URLs and utility functions\n",
                "\n",
                "# Model URLs\n",
                "FACE_MODEL_URL = \"https://huggingface.co/arnabdhar/YOLOv8-Face-Detection/resolve/main/model.pt\"\n",
                "FACE_MODEL_NAME = \"yolov8n-face.pt\"\n",
                "GENDER_PROTO_URL = \"https://huggingface.co/AjaySharma/genderDetection/resolve/main/gender_deploy.prototxt\"\n",
                "GENDER_MODEL_URL = \"https://huggingface.co/AjaySharma/genderDetection/resolve/main/gender_net.caffemodel\"\n",
                "GENDER_PROTO = \"gender_deploy.prototxt\"\n",
                "GENDER_MODEL = \"gender_net.caffemodel\"\n",
                "\n",
                "def download_file(url, filename):\n",
                "    \"\"\"Download a file if it doesn't exist.\"\"\"\n",
                "    if not os.path.exists(filename):\n",
                "        print(f\"‚¨áÔ∏è Downloading {filename}...\")\n",
                "        urllib.request.urlretrieve(url, filename)\n",
                "        print(f\"   ‚úÖ Downloaded\")\n",
                "    else:\n",
                "        print(f\"‚úì Found {filename}\")\n",
                "\n",
                "def setup_models(mode='clip'):\n",
                "    \"\"\"Download required model files.\"\"\"\n",
                "    download_file(FACE_MODEL_URL, FACE_MODEL_NAME)\n",
                "    if mode == 'caffe':\n",
                "        download_file(GENDER_PROTO_URL, GENDER_PROTO)\n",
                "        download_file(GENDER_MODEL_URL, GENDER_MODEL)\n",
                "\n",
                "def blur_region(image, box):\n",
                "    \"\"\"Applies Gaussian blur to bounding box region.\"\"\"\n",
                "    x1, y1, x2, y2 = map(int, box)\n",
                "    h, w = image.shape[:2]\n",
                "    x1, y1 = max(0, x1), max(0, y1)\n",
                "    x2, y2 = min(w, x2), min(h, y2)\n",
                "    roi = image[y1:y2, x1:x2]\n",
                "    if roi.size == 0: return image\n",
                "    ksize = int(max(roi.shape[:2]) // 5) | 1\n",
                "    if ksize <= 1: return image\n",
                "    blurred_roi = cv2.GaussianBlur(roi, (ksize, ksize), 0)\n",
                "    image[y1:y2, x1:x2] = blurred_roi\n",
                "    return image\n",
                "\n",
                "def draw_debug(image, face_box, person_box, gender, conf, is_tracked_female=False, track_id=None):\n",
                "    \"\"\"Draws debug bounding boxes.\"\"\"\n",
                "    if face_box:\n",
                "        fx1, fy1, fx2, fy2 = face_box\n",
                "        color = (255, 0, 0) if gender == 'Male' else (255, 0, 255)\n",
                "        cv2.rectangle(image, (fx1, fy1), (fx2, fy2), color, 2)\n",
                "        cv2.putText(image, f\"{gender} {conf:.2f}\", (fx1, fy1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
                "    if person_box is not None:\n",
                "        px1, py1, px2, py2 = person_box\n",
                "        p_color = (0, 0, 255) if is_tracked_female else (0, 255, 0)\n",
                "        cv2.rectangle(image, (px1, py1), (px2, py2), p_color, 2)\n",
                "        id_label = f\"ID:{track_id}\" if track_id else \"ID:?\"\n",
                "        label = f\"{id_label} FEMALE\" if is_tracked_female else id_label\n",
                "        cv2.putText(image, label, (px1, py1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, p_color, 2)\n",
                "\n",
                "class GlobalTrackManager:\n",
                "    \"\"\"Manages gender votes across frames for consistent classification.\"\"\"\n",
                "    def __init__(self, sensitivity=0.15):\n",
                "        self.track_gender_votes = {}\n",
                "        self.final_decisions = {}\n",
                "        self.sensitivity = sensitivity\n",
                "\n",
                "    def add_vote(self, track_id, gender):\n",
                "        if track_id is None: return\n",
                "        if track_id not in self.track_gender_votes:\n",
                "            self.track_gender_votes[track_id] = {'Male': 0, 'Female': 0}\n",
                "        self.track_gender_votes[track_id][gender] += 1\n",
                "\n",
                "    def finalize(self):\n",
                "        print(\"\\nüìä Finalizing Gender Tracks:\")\n",
                "        for tid, votes in self.track_gender_votes.items():\n",
                "            total = votes['Male'] + votes['Female']\n",
                "            if total == 0: continue\n",
                "            female_ratio = votes['Female'] / total\n",
                "            self.final_decisions[tid] = 'Female' if female_ratio >= self.sensitivity else 'Male'\n",
                "            print(f\"   Track {tid}: {votes} ‚Üí {self.final_decisions[tid]}\")\n",
                "\n",
                "print(\"‚úÖ Helper functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üé¨ Define Video Processing Function { display-mode: \"form\" }\n",
                "\n",
                "def process_video(input_path, output_path, conf_threshold=0.25, start_seconds=0,\n",
                "                  duration_seconds=None, debug=False, mode='clip', progress_callback=None, **kwargs):\n",
                "    \"\"\"Process video: detect and blur females using two-pass approach.\"\"\"\n",
                "    setup_models(mode=mode)\n",
                "    \n",
                "    print(f\"\\nüîß Loading models (mode={mode})...\")\n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    \n",
                "    face_model = YOLO(FACE_MODEL_NAME)\n",
                "    person_model = YOLO(\"yolov8n.pt\")\n",
                "    \n",
                "    gender_net = None\n",
                "    clip_model, clip_preprocess, clip_tokenizer = None, None, None\n",
                "    \n",
                "    if mode == 'caffe':\n",
                "        gender_net = cv2.dnn.readNet(GENDER_PROTO, GENDER_MODEL)\n",
                "    elif mode == 'clip':\n",
                "        clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
                "            'ViT-B-32', pretrained='laion2b_s34b_b79k', device=device)\n",
                "        clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
                "    \n",
                "    gender_list = ['Male', 'Female']\n",
                "    global_tracker = GlobalTrackManager(sensitivity=kwargs.get('sensitivity', 0.15))\n",
                "    \n",
                "    cap = cv2.VideoCapture(input_path)\n",
                "    if not cap.isOpened():\n",
                "        print(f\"‚ùå Error opening video {input_path}\")\n",
                "        return\n",
                "\n",
                "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
                "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "    video_duration = total_frames / fps\n",
                "    \n",
                "    if start_seconds < 0:\n",
                "        start_seconds = max(0, video_duration + start_seconds)\n",
                "    start_frame = int(start_seconds * fps)\n",
                "    max_frames = int(duration_seconds * fps) if duration_seconds else (total_frames - start_frame)\n",
                "    if start_frame + max_frames > total_frames:\n",
                "        max_frames = total_frames - start_frame\n",
                "    \n",
                "    print(f\"üìΩÔ∏è Processing: {start_seconds:.1f}s to {start_seconds + (max_frames/fps):.1f}s ({max_frames} frames)\")\n",
                "    \n",
                "    # --- PASS 1: Analysis ---\n",
                "    print(f\"\\nüîç PASS 1: Analyzing...\")\n",
                "    frame_count = 0\n",
                "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
                "    \n",
                "    while frame_count < max_frames:\n",
                "        ret, frame = cap.read()\n",
                "        if not ret: break\n",
                "        \n",
                "        results = person_model.track(frame, verbose=False, classes=[0], persist=True, conf=conf_threshold)\n",
                "        \n",
                "        if results and results[0].boxes.id is not None:\n",
                "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
                "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
                "            \n",
                "            if mode == 'clip':\n",
                "                text_prompts = clip_tokenizer([\"a photo of a man\", \"a photo of a woman\"]).to(device)\n",
                "                for i, (px1, py1, px2, py2) in enumerate(boxes):\n",
                "                    person_img = frame[max(0,py1):min(height,py2), max(0,px1):min(width,px2)]\n",
                "                    if person_img.size > 0:\n",
                "                        person_pil = Image.fromarray(cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB))\n",
                "                        image_input = clip_preprocess(person_pil).unsqueeze(0).to(device)\n",
                "                        with torch.no_grad():\n",
                "                            image_features = clip_model.encode_image(image_input)\n",
                "                            text_features = clip_model.encode_text(text_prompts)\n",
                "                            probs = (100.0 * image_features @ text_features.T).softmax(dim=-1).cpu().numpy()[0]\n",
                "                            gender = 'Male' if probs[0] > probs[1] else 'Female'\n",
                "                            global_tracker.add_vote(ids[i], gender)\n",
                "            else:\n",
                "                face_results = face_model(frame, verbose=False, conf=conf_threshold)\n",
                "                for result in face_results:\n",
                "                    for box in result.boxes:\n",
                "                        bx1, by1, bx2, by2 = map(int, box.xyxy[0])\n",
                "                        face_cx, face_cy = (bx1+bx2)/2, (by1+by2)/2\n",
                "                        face_img = frame[max(0,by1):min(height,by2), max(0,bx1):min(width,bx2)]\n",
                "                        if face_img.size > 0:\n",
                "                            blob = cv2.dnn.blobFromImage(face_img, 1.0, (227, 227),\n",
                "                                (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
                "                            gender_net.setInput(blob)\n",
                "                            preds = gender_net.forward()\n",
                "                            gender = gender_list[preds[0].argmax()]\n",
                "                            for i, (px1, py1, px2, py2) in enumerate(boxes):\n",
                "                                h_margin = (py2 - py1) * 0.2\n",
                "                                if px1 <= face_cx <= px2 and (py1 - h_margin) <= face_cy <= py2:\n",
                "                                    global_tracker.add_vote(ids[i], gender)\n",
                "                                    break\n",
                "        \n",
                "        frame_count += 1\n",
                "        if frame_count % 30 == 0:\n",
                "            pct = int(frame_count / max_frames * 50)\n",
                "            print(f\"   Pass 1: {frame_count}/{max_frames} ({pct}%)\", end='\\r')\n",
                "            if progress_callback:\n",
                "                progress_callback(pct)\n",
                "\n",
                "    global_tracker.finalize()\n",
                "\n",
                "    # --- PASS 2: Rendering ---\n",
                "    print(f\"\\nüé¨ PASS 2: Rendering...\")\n",
                "    person_model = YOLO(\"yolov8n.pt\")\n",
                "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
                "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
                "    \n",
                "    frame_count = 0\n",
                "    while frame_count < max_frames:\n",
                "        ret, frame = cap.read()\n",
                "        if not ret: break\n",
                "        \n",
                "        results = person_model.track(frame, verbose=False, classes=[0], persist=True, conf=conf_threshold)\n",
                "        \n",
                "        if results and results[0].boxes.id is not None:\n",
                "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
                "            ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
                "            \n",
                "            for box, track_id in zip(boxes, ids):\n",
                "                gender_decision = global_tracker.final_decisions.get(track_id, 'Male')\n",
                "                if gender_decision == 'Female':\n",
                "                    if debug:\n",
                "                        draw_debug(frame, None, box, None, 1.0, True, track_id=track_id)\n",
                "                    else:\n",
                "                        frame = blur_region(frame, box)\n",
                "                elif debug:\n",
                "                    draw_debug(frame, None, box, None, 0.0, False, track_id=track_id)\n",
                "\n",
                "        out.write(frame)\n",
                "        frame_count += 1\n",
                "        if frame_count % 30 == 0:\n",
                "            pct = 50 + int(frame_count / max_frames * 50)\n",
                "            print(f\"   Pass 2: {frame_count}/{max_frames} ({pct}%)\", end='\\r')\n",
                "            if progress_callback:\n",
                "                progress_callback(pct)\n",
                "            \n",
                "    cap.release()\n",
                "    out.release()\n",
                "    print(\"\\n‚úÖ Video processing complete!\")\n",
                "\n",
                "print(\"‚úÖ Video processing function defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üéµ Define Audio Processing Function { display-mode: \"form\" }\n",
                "\n",
                "def find_demucs():\n",
                "    \"\"\"Locate the demucs executable.\"\"\"\n",
                "    candidate = Path(sys.executable).parent / \"demucs\"\n",
                "    if candidate.exists() and os.access(candidate, os.X_OK):\n",
                "        return str(candidate)\n",
                "    path_demucs = shutil.which(\"demucs\")\n",
                "    return path_demucs if path_demucs else \"demucs\"\n",
                "\n",
                "def remove_music(input_path, output_path=None, model=\"htdemucs\"):\n",
                "    \"\"\"Separates vocals from audio using Demucs.\"\"\"\n",
                "    input_path = Path(input_path).resolve()\n",
                "    if not input_path.exists():\n",
                "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
                "\n",
                "    print(f\"\\nüéµ Processing audio: {input_path.name}\")\n",
                "    \n",
                "    demucs_cmd = find_demucs()\n",
                "    env = os.environ.copy()\n",
                "    try:\n",
                "        add_paths()\n",
                "        env[\"PATH\"] = os.environ[\"PATH\"]\n",
                "    except ImportError:\n",
                "        pass\n",
                "\n",
                "    cmd = [demucs_cmd, \"--two-stems=vocals\", \"-n\", model, str(input_path)]\n",
                "    print(f\"   Running Demucs (this may take several minutes)...\")\n",
                "    subprocess.run(cmd, check=True, env=env)\n",
                "    \n",
                "    filename_stem = input_path.stem\n",
                "    output_root = Path(\"separated\") / model\n",
                "    candidate_dir = output_root / filename_stem\n",
                "    vocals_file = candidate_dir / \"vocals.wav\"\n",
                "    \n",
                "    if not vocals_file.exists():\n",
                "        if output_root.exists():\n",
                "            for child in output_root.iterdir():\n",
                "                if child.is_dir():\n",
                "                    vocals_file = child / \"vocals.wav\"\n",
                "                    if vocals_file.exists():\n",
                "                        candidate_dir = child\n",
                "                        break\n",
                "    \n",
                "    if not vocals_file.exists():\n",
                "        raise RuntimeError(f\"Could not locate separated output\")\n",
                "\n",
                "    if output_path:\n",
                "        output_path = Path(output_path).resolve()\n",
                "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "        shutil.move(str(vocals_file), str(output_path))\n",
                "        target_file = output_path\n",
                "    else:\n",
                "        target_file = input_path.with_name(f\"{input_path.stem}_vocals.wav\")\n",
                "        shutil.move(str(vocals_file), str(target_file))\n",
                "\n",
                "    try:\n",
                "        shutil.rmtree(candidate_dir)\n",
                "        output_root.rmdir()\n",
                "        output_root.parent.rmdir()\n",
                "    except OSError:\n",
                "        pass\n",
                "\n",
                "    print(f\"   ‚úÖ Vocals saved to: {target_file}\")\n",
                "    return str(target_file)\n",
                "\n",
                "def combine_video_audio(video_path, audio_path, output_path):\n",
                "    \"\"\"Combines video and audio using ffmpeg.\"\"\"\n",
                "    print(f\"\\nüîó Merging video and audio...\")\n",
                "    add_paths()\n",
                "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-i\", audio_path,\n",
                "           \"-c:v\", \"copy\", \"-c:a\", \"aac\", \"-map\", \"0:v:0\", \"-map\", \"1:a:0\",\n",
                "           \"-shortest\", output_path]\n",
                "    try:\n",
                "        subprocess.run(cmd, check=True, capture_output=True)\n",
                "        print(\"   ‚úÖ Merge complete!\")\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"   ‚ö†Ô∏è Merge failed, saving video only\")\n",
                "        shutil.move(video_path, output_path)\n",
                "\n",
                "print(\"‚úÖ Audio processing functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Define Main Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üöÄ Define Halalify Pipeline { display-mode: \"form\" }\n",
                "\n",
                "def halalify(input_path, output_path, audio_path=None, progress_callback=None, **kwargs):\n",
                "    \"\"\"Full pipeline: Blur females AND remove music.\"\"\"\n",
                "    temp_blurred = \"temp_blurred_video.mp4\"\n",
                "    temp_vocals = \"temp_vocals.wav\"\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"üé• HALALIFICATOR PROCESSING PIPELINE\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # 1. Process Video\n",
                "    print(\"\\nüìπ STEP 1: Processing Video\")\n",
                "    print(\"-\" * 40)\n",
                "    process_video(input_path, temp_blurred,\n",
                "                  conf_threshold=kwargs.get('conf', 0.25),\n",
                "                  start_seconds=kwargs.get('start', 0),\n",
                "                  duration_seconds=kwargs.get('duration'),\n",
                "                  debug=kwargs.get('debug', False),\n",
                "                  sensitivity=kwargs.get('sensitivity', 0.15),\n",
                "                  mode=kwargs.get('mode', 'clip'),\n",
                "                  progress_callback=progress_callback)\n",
                "    \n",
                "    # 2. Process Audio\n",
                "    print(\"\\nüéµ STEP 2: Processing Audio\")\n",
                "    print(\"-\" * 40)\n",
                "    try:\n",
                "        audio_source = audio_path if audio_path else input_path\n",
                "        remove_music(audio_source, output_path=temp_vocals)\n",
                "        combine_video_audio(temp_blurred, temp_vocals, output_path)\n",
                "    except Exception as e:\n",
                "        print(f\"\\n‚ö†Ô∏è Audio processing failed: {e}\")\n",
                "        shutil.move(temp_blurred, output_path)\n",
                "    \n",
                "    # Cleanup\n",
                "    for f in [temp_blurred, temp_vocals]:\n",
                "        if os.path.exists(f): os.remove(f)\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(f\"‚úÖ COMPLETE: {output_path}\")\n",
                "    print(f\"   File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
                "    print(\"=\" * 60)\n",
                "\n",
                "print(\"‚úÖ Halalify pipeline defined!\")\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üéâ SETUP COMPLETE! Proceed to upload your video.\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6Ô∏è‚É£ Upload Your Video\n",
                "\n",
                "Choose one of the methods below to get your video into Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üì§ Option A: Upload from Computer { display-mode: \"form\" }\n",
                "#@markdown Click the **Choose Files** button below to upload your video.\n",
                "\n",
                "INPUT_VIDEO = None\n",
                "\n",
                "if IN_COLAB:\n",
                "    from google.colab import files\n",
                "    print(\"üì§ Click the button below to upload your video file:\")\n",
                "    print(\"   (Supported: MP4, MKV, AVI, MOV, WebM)\\n\")\n",
                "    uploaded = files.upload()\n",
                "    \n",
                "    if uploaded:\n",
                "        INPUT_VIDEO = list(uploaded.keys())[0]\n",
                "        file_size = os.path.getsize(INPUT_VIDEO) / (1024*1024)\n",
                "        print(f\"\\n‚úÖ Uploaded: {INPUT_VIDEO} ({file_size:.2f} MB)\")\n",
                "    else:\n",
                "        print(\"‚ùå No file uploaded\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Not in Colab. Use Option B or C instead.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üåê Option B: Download from URL { display-mode: \"form\" }\n",
                "#@markdown Enter a direct download URL for your video:\n",
                "\n",
                "video_url = \"\"  #@param {type:\"string\"}\n",
                "output_filename = \"downloaded_video.mp4\"  #@param {type:\"string\"}\n",
                "\n",
                "if video_url:\n",
                "    print(f\"‚¨áÔ∏è Downloading from: {video_url[:50]}...\")\n",
                "    !wget -q -O \"{output_filename}\" \"{video_url}\"\n",
                "    \n",
                "    if os.path.exists(output_filename):\n",
                "        file_size = os.path.getsize(output_filename) / (1024*1024)\n",
                "        INPUT_VIDEO = output_filename\n",
                "        print(f\"\\n‚úÖ Downloaded: {INPUT_VIDEO} ({file_size:.2f} MB)\")\n",
                "    else:\n",
                "        print(\"‚ùå Download failed\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è Enter a URL above and run this cell to download\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üì∫ Option C: Download from YouTube { display-mode: \"form\" }\n",
                "#@markdown Enter a YouTube URL to download:\n",
                "\n",
                "youtube_url = \"\"  #@param {type:\"string\"}\n",
                "output_name = \"youtube_video\"  #@param {type:\"string\"}\n",
                "\n",
                "if youtube_url:\n",
                "    print(\"üì¶ Installing yt-dlp...\")\n",
                "    !pip install -q yt-dlp\n",
                "    \n",
                "    print(f\"\\n‚¨áÔ∏è Downloading from YouTube...\")\n",
                "    !yt-dlp -f \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best\" \\\n",
                "        -o \"{output_name}.%(ext)s\" \"{youtube_url}\" --no-playlist\n",
                "    \n",
                "    # Find the downloaded file\n",
                "    for ext in ['mp4', 'mkv', 'webm']:\n",
                "        candidate = f\"{output_name}.{ext}\"\n",
                "        if os.path.exists(candidate):\n",
                "            INPUT_VIDEO = candidate\n",
                "            file_size = os.path.getsize(INPUT_VIDEO) / (1024*1024)\n",
                "            print(f\"\\n‚úÖ Downloaded: {INPUT_VIDEO} ({file_size:.2f} MB)\")\n",
                "            break\n",
                "    else:\n",
                "        print(\"‚ùå Download failed or file format not found\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è Enter a YouTube URL above and run this cell to download\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ‚úÖ Verify Selected Video { display-mode: \"form\" }\n",
                "#@markdown Run this to confirm which video will be processed:\n",
                "\n",
                "# You can also manually set the video path here:\n",
                "manual_path = \"\"  #@param {type:\"string\"}\n",
                "\n",
                "if manual_path:\n",
                "    INPUT_VIDEO = manual_path\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"üìπ SELECTED VIDEO\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "if INPUT_VIDEO and os.path.exists(INPUT_VIDEO):\n",
                "    file_size = os.path.getsize(INPUT_VIDEO) / (1024*1024)\n",
                "    print(f\"\\n‚úÖ Video: {INPUT_VIDEO}\")\n",
                "    print(f\"   Size: {file_size:.2f} MB\")\n",
                "    \n",
                "    # Get video info\n",
                "    cap = cv2.VideoCapture(INPUT_VIDEO)\n",
                "    if cap.isOpened():\n",
                "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
                "        frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "        duration = frames / fps if fps > 0 else 0\n",
                "        print(f\"   Duration: {duration:.1f} seconds ({frames} frames)\")\n",
                "        print(f\"   Resolution: {width}x{height} @ {fps:.1f} FPS\")\n",
                "        cap.release()\n",
                "    print(\"\\n‚úÖ Ready to proceed to configuration!\")\n",
                "else:\n",
                "    print(\"\\n‚ùå No video selected or file not found!\")\n",
                "    print(\"   Please use one of the upload options above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7Ô∏è‚É£ Configure Processing Options\n",
                "\n",
                "Adjust these settings before running the processor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ‚öôÔ∏è Processing Configuration { display-mode: \"form\" }\n",
                "\n",
                "#@markdown ### Output Settings\n",
                "output_filename = \"halalified_output.mp4\"  #@param {type:\"string\"}\n",
                "\n",
                "#@markdown ---\n",
                "#@markdown ### Gender Detection Mode\n",
                "detection_mode = \"clip\"  #@param [\"clip\", \"caffe\"]\n",
                "#@markdown - **clip**: Uses CLIP model on full body (recommended, more robust)\n",
                "#@markdown - **caffe**: Uses face-based detection (requires clear face visibility)\n",
                "\n",
                "#@markdown ---\n",
                "#@markdown ### Detection Parameters\n",
                "confidence_threshold = 0.25  #@param {type:\"slider\", min:0.1, max:0.9, step:0.05}\n",
                "#@markdown Confidence threshold for person detection (lower = more detections)\n",
                "\n",
                "sensitivity = 0.15  #@param {type:\"slider\", min:0.05, max:0.5, step:0.05}\n",
                "#@markdown Gender sensitivity (lower = more aggressive blurring)\n",
                "\n",
                "#@markdown ---\n",
                "#@markdown ### Processing Range (for testing)\n",
                "start_time = 0  #@param {type:\"number\"}\n",
                "#@markdown Start from this second (use negative for end, e.g., -5 for last 5 seconds)\n",
                "\n",
                "duration = 0  #@param {type:\"number\"}\n",
                "#@markdown Duration in seconds (0 = full video)\n",
                "\n",
                "#@markdown ---\n",
                "#@markdown ### Debug Mode\n",
                "debug_mode = False  #@param {type:\"boolean\"}\n",
                "#@markdown If enabled, draws bounding boxes instead of blurring (for testing)\n",
                "\n",
                "# Store configuration\n",
                "OUTPUT_VIDEO = f\"outputs/{output_filename}\"\n",
                "MODE = detection_mode\n",
                "CONF_THRESHOLD = confidence_threshold\n",
                "SENSITIVITY = sensitivity\n",
                "START_TIME = start_time\n",
                "DURATION = duration if duration > 0 else None\n",
                "DEBUG_MODE = debug_mode\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"‚öôÔ∏è CONFIGURATION SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nüìπ Input:  {INPUT_VIDEO}\")\n",
                "print(f\"üìÅ Output: {OUTPUT_VIDEO}\")\n",
                "print(f\"\\nüîß Mode: {MODE}\")\n",
                "print(f\"   Confidence: {CONF_THRESHOLD}\")\n",
                "print(f\"   Sensitivity: {SENSITIVITY}\")\n",
                "print(f\"\\n‚è±Ô∏è Range: {START_TIME}s to {'end' if DURATION is None else f'{START_TIME + DURATION}s'}\")\n",
                "print(f\"üêõ Debug: {DEBUG_MODE}\")\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"‚úÖ Configuration saved! Ready to process.\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8Ô∏è‚É£ Run Processing\n",
                "\n",
                "This will process your video. Time depends on video length and GPU availability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üöÄ Run Halalificator! { display-mode: \"form\" }\n",
                "#@markdown Click the play button to start processing.\n",
                "#@markdown \n",
                "#@markdown **Estimated time**: ~2-5 minutes per minute of video (with GPU)\n",
                "\n",
                "import time\n",
                "\n",
                "if not INPUT_VIDEO or not os.path.exists(INPUT_VIDEO):\n",
                "    print(\"‚ùå ERROR: No input video selected!\")\n",
                "    print(\"   Please upload a video in Section 6 first.\")\n",
                "else:\n",
                "    start_time_process = time.time()\n",
                "    \n",
                "    halalify(\n",
                "        INPUT_VIDEO,\n",
                "        OUTPUT_VIDEO,\n",
                "        mode=MODE,\n",
                "        conf=CONF_THRESHOLD,\n",
                "        sensitivity=SENSITIVITY,\n",
                "        start=START_TIME,\n",
                "        duration=DURATION,\n",
                "        debug=DEBUG_MODE\n",
                "    )\n",
                "    \n",
                "    elapsed = time.time() - start_time_process\n",
                "    print(f\"\\n‚è±Ô∏è Total processing time: {elapsed/60:.1f} minutes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9Ô∏è‚É£ Preview & Download Result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üé¨ Preview Output Video { display-mode: \"form\" }\n",
                "#@markdown Plays the processed video inline (for videos under 50MB)\n",
                "\n",
                "if os.path.exists(OUTPUT_VIDEO):\n",
                "    file_size = os.path.getsize(OUTPUT_VIDEO) / (1024*1024)\n",
                "    print(f\"üìÅ Output: {OUTPUT_VIDEO}\")\n",
                "    print(f\"   Size: {file_size:.2f} MB\\n\")\n",
                "    \n",
                "    if IN_COLAB and file_size < 50:\n",
                "        from base64 import b64encode\n",
                "        print(\"üé¨ Video Preview:\")\n",
                "        mp4 = open(OUTPUT_VIDEO, 'rb').read()\n",
                "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
                "        display(HTML(f'''\n",
                "        <video width=\"640\" controls>\n",
                "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
                "        </video>\n",
                "        '''))\n",
                "    elif file_size >= 50:\n",
                "        print(\"‚ö†Ô∏è Video too large for inline preview. Please download to view.\")\n",
                "    else:\n",
                "        print(f\"‚ÑπÔ∏è Open {OUTPUT_VIDEO} with your video player to preview.\")\n",
                "else:\n",
                "    print(\"‚ùå Output video not found. Please run processing first (Section 8).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üì• Download Output Video { display-mode: \"form\" }\n",
                "#@markdown Downloads the processed video to your computer.\n",
                "\n",
                "if os.path.exists(OUTPUT_VIDEO):\n",
                "    if IN_COLAB:\n",
                "        from google.colab import files\n",
                "        print(\"üì• Starting download...\")\n",
                "        files.download(OUTPUT_VIDEO)\n",
                "        print(\"\\n‚úÖ Download initiated! Check your browser downloads.\")\n",
                "    else:\n",
                "        print(f\"üìÅ Output saved to: {os.path.abspath(OUTPUT_VIDEO)}\")\n",
                "else:\n",
                "    print(\"‚ùå Output video not found. Please run processing first (Section 8).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üîß Advanced Options\n",
                "\n",
                "Use these if you want to run only part of the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üìπ Video Only (No Audio Processing) { display-mode: \"form\" }\n",
                "#@markdown Blurs females but skips music removal (faster)\n",
                "\n",
                "video_only_output = \"outputs/blurred_only.mp4\"  #@param {type:\"string\"}\n",
                "run_video_only = False  #@param {type:\"boolean\"}\n",
                "\n",
                "if run_video_only and INPUT_VIDEO:\n",
                "    process_video(\n",
                "        INPUT_VIDEO,\n",
                "        video_only_output,\n",
                "        conf_threshold=CONF_THRESHOLD,\n",
                "        start_seconds=START_TIME,\n",
                "        duration_seconds=DURATION,\n",
                "        debug=DEBUG_MODE,\n",
                "        sensitivity=SENSITIVITY,\n",
                "        mode=MODE\n",
                "    )\n",
                "    print(f\"\\n‚úÖ Video saved to: {video_only_output}\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è Enable 'run_video_only' checkbox and run this cell to process video only.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üéµ Audio Only (Music Removal) { display-mode: \"form\" }\n",
                "#@markdown Removes music from audio/video without any video processing\n",
                "\n",
                "audio_only_output = \"outputs/vocals_only.wav\"  #@param {type:\"string\"}\n",
                "run_audio_only = False  #@param {type:\"boolean\"}\n",
                "\n",
                "if run_audio_only and INPUT_VIDEO:\n",
                "    remove_music(INPUT_VIDEO, output_path=audio_only_output)\n",
                "    print(f\"\\n‚úÖ Vocals saved to: {audio_only_output}\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è Enable 'run_audio_only' checkbox and run this cell to extract vocals only.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚ùì Troubleshooting\n",
                "\n",
                "| Issue | Solution |\n",
                "|-------|----------|\n",
                "| **No GPU available** | Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU` |\n",
                "| **Out of memory** | Use shorter duration, reduce video resolution, or restart runtime |\n",
                "| **Slow processing** | Ensure GPU is enabled; CLIP mode is slower but more accurate |\n",
                "| **Audio processing fails** | May happen if no audio track; video will still be processed |\n",
                "| **Download doesn't start** | Try right-clicking the download link or use Files panel (left sidebar) |"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": [],
            "collapsed_sections": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}